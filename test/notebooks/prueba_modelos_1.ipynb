{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2164d84c",
   "metadata": {},
   "source": [
    "## Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4436d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa220cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53895e87",
   "metadata": {},
   "source": [
    "*Keras*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0a781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, MaxPool2D, Flatten\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5278e8d",
   "metadata": {},
   "source": [
    "*Sklearn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c703466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538eac1c",
   "metadata": {},
   "source": [
    "*Datos + Viz*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdac9645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78839b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c436c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e23690",
   "metadata": {},
   "source": [
    "HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a127868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a128ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = pd.read_csv('C:/Users/swatc/Desktop/UNI/TFM/TFM/artifacts/data/calc.csv')\n",
    "mass = pd.read_csv('C:/Users/swatc/Desktop/UNI/TFM/TFM/artifacts/data/mass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca7cda39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = calc.loc[0, 'image_file_path_red']  # acceder a la ruta del archivo\n",
    "image = mpimg.imread(image_path)  # leer la imagen\n",
    "\n",
    "plt.imshow(image, cmap='gray')  # o sin cmap si es RGB\n",
    "plt.axis('off')  # opcional, para ocultar ejes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31f32d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   type                              image_file_path_red  target\n",
      "0  calc  processed_images\\calc\\image_file_path\\1-188.jpg       1\n",
      "1  calc  processed_images\\calc\\image_file_path\\1-189.jpg       1\n",
      "2  calc  processed_images\\calc\\image_file_path\\1-190.jpg       0\n",
      "3  calc  processed_images\\calc\\image_file_path\\1-191.jpg       0\n",
      "4  calc  processed_images\\calc\\image_file_path\\1-192.jpg       0\n",
      "target\n",
      "0    1589\n",
      "1    1268\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "mass    1592\n",
      "calc    1265\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Crear la columna 'target' con categor√≠a 1=maligno, 0 = benigno\n",
    "calc['target'] = calc['pathology'].apply(lambda x: 1 if x == 'MALIGNANT' else 0)\n",
    "mass['target'] = mass['pathology'].apply(lambda x: 1 if x == 'MALIGNANT' else 0)\n",
    "\n",
    "# A√±adir columna 'type'\n",
    "calc['type'] = 'calc'\n",
    "mass['type'] = 'mass'\n",
    "\n",
    "# Seleccionar solo columnas necesarias y unificarlas\n",
    "df = pd.concat([\n",
    "    calc[['type', 'image_file_path_red', 'target']],\n",
    "    mass[['type', 'image_file_path_red', 'target']]\n",
    "], ignore_index=True)\n",
    "\n",
    "# Verificar resultado\n",
    "print(df.head())\n",
    "print(df['target'].value_counts())\n",
    "print(df['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da069a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"image_file_path_red\": \"image\", \"target\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f07b48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>calc</td>\n",
       "      <td>processed_images\\calc\\image_file_path\\1-188.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>calc</td>\n",
       "      <td>processed_images\\calc\\image_file_path\\1-189.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calc</td>\n",
       "      <td>processed_images\\calc\\image_file_path\\1-190.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>calc</td>\n",
       "      <td>processed_images\\calc\\image_file_path\\1-191.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>calc</td>\n",
       "      <td>processed_images\\calc\\image_file_path\\1-192.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            image  label\n",
       "0  calc  processed_images\\calc\\image_file_path\\1-188.jpg      1\n",
       "1  calc  processed_images\\calc\\image_file_path\\1-189.jpg      1\n",
       "2  calc  processed_images\\calc\\image_file_path\\1-190.jpg      0\n",
       "3  calc  processed_images\\calc\\image_file_path\\1-191.jpg      0\n",
       "4  calc  processed_images\\calc\\image_file_path\\1-192.jpg      0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66b559e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/swatc/Desktop/UNI/TFM/TFM/artifacts/data/df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0936acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir a Hugging Face Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e20ccea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ca89e",
   "metadata": {},
   "source": [
    "## ViTForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "397cd837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo y preprocesador\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(checkpoint)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e02dec",
   "metadata": {},
   "source": [
    "Este mensaje es normal, ahora se debe hacer fine-tune para que el modelo para que aprenda a clasificar correctamente las 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9929f75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed7d0c9a0c545d1bc87cafd65157775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# carga + preprocesamiento\n",
    "def transform(example):\n",
    "    image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs[\"label\"] = example[\"label\"]\n",
    "    return {\"pixel_values\": inputs[\"pixel_values\"][0], \"label\": inputs[\"label\"]}\n",
    "\n",
    "# Preprocesar dataset\n",
    "hf_dataset = hf_dataset.map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0e2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "split = hf_dataset.train_test_split(test_size=0.2)\n",
    "train_ds = split[\"train\"]\n",
    "test_ds = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b89bf685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "760a0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810eff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0e6337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©trica\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5227c642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1430' max='1430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1430/1430 1:24:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.826200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.745800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.685100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.648600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.755600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.773800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.717400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.690300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.741700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.702400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.736100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.696400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.702800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.728900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.732000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.676200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.684600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.663100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.638300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.724700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.689200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.653800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.652700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.641100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.653900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.624700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.680200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.665100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.646700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.643700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.655200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.657100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.706600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.604000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.677000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.603200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.560900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.605500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.588700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.739700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.594700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.579900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.587600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.558700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1430, training_loss=0.6756643602064439, metrics={'train_runtime': 5050.2846, 'train_samples_per_second': 2.262, 'train_steps_per_second': 0.283, 'total_flos': 8.853459813467136e+17, 'train_loss': 0.6756643602064439, 'epoch': 5.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0942e29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7822157740592957, 'eval_accuracy': 0.5052447552447552, 'eval_runtime': 151.7957, 'eval_samples_per_second': 3.768, 'eval_steps_per_second': 0.474, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73423e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Benigno       0.53      0.68      0.60       310\n",
      "     Maligno       0.44      0.30      0.36       262\n",
      "\n",
      "    accuracy                           0.51       572\n",
      "   macro avg       0.49      0.49      0.48       572\n",
      "weighted avg       0.49      0.51      0.49       572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtener las predicciones con etiquetas reales, para luego hacer an√°lisis m√°s detallado\n",
    "\n",
    "predictions_output = trainer.predict(test_ds)\n",
    "\n",
    "# Obtener predicciones y etiquetas\n",
    "pred_logits = predictions_output.predictions  # salida del modelo (logits)\n",
    "pred_labels = np.argmax(pred_logits, axis=1)  # etiquetas predichas (0 o 1)\n",
    "true_labels = predictions_output.label_ids    # etiquetas reales\n",
    "\n",
    "# Calcular accuracy (ya lo hace trainer, pero si quieres hacerlo manual)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# M√°s detalle: reporte clasificaci√≥n\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"Benigno\", \"Maligno\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900c641",
   "metadata": {},
   "source": [
    "Un accuracy tan cercano a 0,5 es pr√°cticamente un clasificador aleatorio. Por otro lado, hay una diferencia grande en recall (0.68 en benigno y 0.30 para maligno), por lo que parece que el modelo es m√°s propenso a precedir benigno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f4f0b3",
   "metadata": {},
   "source": [
    "Posibles mejoras para el modelo: \n",
    "\n",
    "1. Balancear clases en entrenamiento: weighted loss para penalizar m√°s los errores en clase Maligno.\n",
    "\n",
    "2. Aumentar datos (Data Augmentation): transformaciones (rotaci√≥n, zoom, flips) \n",
    "\n",
    "3. Ajustar hiperpar√°metros: aumentar epochs por ejemplo\n",
    "\n",
    "4. Probar otros modelos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24ed55",
   "metadata": {},
   "source": [
    "1. Weight loss para balancear clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f677550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights) #para penalizar m√°s los errores en la clase minoritaria.\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Crear tensor con pesos: Clase 0 (benigno): peso 1.0, Clase 1 (maligno): peso 3.0\n",
    "class_weights = torch.tensor([1.0, 3.0]).to(model.device) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9068676",
   "metadata": {},
   "source": [
    "2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca30bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Definir transformaciones para data augmentation (aplicarlas solo en train)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),  # rotar hasta 15 grados\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # zoom y recorte\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Para validaci√≥n solo normalizaci√≥n + ToTensor\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Modifica la funci√≥n transform para aplicar las transformaciones:\n",
    "\n",
    "def transform_train(example):\n",
    "    image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    image = train_transforms(image)\n",
    " \n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs[\"label\"] = example[\"label\"]\n",
    "    return {\"pixel_values\": inputs[\"pixel_values\"][0], \"label\": inputs[\"label\"]}\n",
    "\n",
    "def transform_val(example):\n",
    "    image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    image = val_transforms(image)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs[\"label\"] = example[\"label\"]\n",
    "    return {\"pixel_values\": inputs[\"pixel_values\"][0], \"label\": inputs[\"label\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a438bb",
   "metadata": {},
   "source": [
    "3. Ajustar hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91b2501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,  # antes 5, ahora m√°s\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12eb18b140fb4bcfb6871379ebe69d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb2cb6dbcd4b139f3ca33be3c2d1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2857 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4290' max='4290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4290/4290 4:14:33, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.706400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.615900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.712900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.731600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.664700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.648200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.658800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.736500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.631400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.651700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.657900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.643500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.689100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.686500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.666100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.706300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.638200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.614400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.646600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.590600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.610400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.635800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.613200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.640200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.560200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.722400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.617900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.609900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.768100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.544900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.717300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.564100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.676900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.617600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.566700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.586300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.704000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.652900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.601200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.624600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.643200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.518700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.392300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.443400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.404200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>0.478200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>0.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>0.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>0.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.312800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>0.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>0.172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>0.557900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>0.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.169200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>0.229400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>0.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>0.214100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>0.152400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>0.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>0.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.074900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>0.088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2530</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2570</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2590</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2630</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2670</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2690</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2710</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2730</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2770</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2830</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2870</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2890</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2930</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2990</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3030</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3070</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3090</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3230</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3310</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3370</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3390</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3470</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3530</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3570</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3630</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3670</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3770</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3790</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3830</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3890</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3930</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3990</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4010</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4030</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4070</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4090</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4270</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4290, training_loss=0.25954946779523796, metrics={'train_runtime': 15278.3833, 'train_samples_per_second': 2.243, 'train_steps_per_second': 0.281, 'total_flos': 2.656037944040141e+18, 'train_loss': 0.25954946779523796, 'epoch': 15.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    ViTForImageClassification,\n",
    "    ViTImageProcessor,\n",
    ")\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset  # solo si usas Hugging Face datasets\n",
    "from PIL import Image\n",
    "import evaluate\n",
    "\n",
    "# 1. Configuraci√≥n del modelo y procesador\n",
    "checkpoint = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(checkpoint)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# 2. Data Augmentation y transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "def transform_train(example):\n",
    "    image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    image = train_transforms(image)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs[\"label\"] = example[\"label\"]\n",
    "    return {\n",
    "        \"pixel_values\": inputs[\"pixel_values\"][0],  # quitar batch dim extra\n",
    "        \"label\": inputs[\"label\"]\n",
    "    }\n",
    "\n",
    "def transform_val(example):\n",
    "    image = Image.open(example[\"image\"]).convert(\"RGB\")\n",
    "    image = val_transforms(image)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs[\"label\"] = example[\"label\"]\n",
    "    return {\n",
    "        \"pixel_values\": inputs[\"pixel_values\"][0],\n",
    "        \"label\": inputs[\"label\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# 4. Aplicar transformaciones\n",
    "hf_dataset_train = hf_dataset.map(transform_train)\n",
    "hf_dataset_val = hf_dataset.map(transform_val)\n",
    "\n",
    "# 5. Dividir el dataset para train/test\n",
    "split = hf_dataset_train.train_test_split(test_size=0.2)\n",
    "train_ds = split[\"train\"]\n",
    "test_ds = split[\"test\"]\n",
    "\n",
    "# 6. Definir clase Trainer personalizada con pesos\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 7. Definir pesos para clases desbalanceadas (ajusta seg√∫n tus datos reales)\n",
    "class_weights = torch.tensor([1.0, 3.0]).to(model.device)\n",
    "\n",
    "# 8. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "\n",
    ")\n",
    "\n",
    "# 9. M√©tricas\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# 10. Entrenador\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "# 11. Entrenamiento\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cce9095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.815617084503174, 'eval_accuracy': 0.5122377622377622, 'eval_runtime': 105.413, 'eval_samples_per_second': 5.426, 'eval_steps_per_second': 0.683, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results_2 = trainer.evaluate()\n",
    "print(eval_results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f132ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Benigno       0.56      0.64      0.60       325\n",
      "     Maligno       0.42      0.34      0.38       247\n",
      "\n",
      "    accuracy                           0.51       572\n",
      "   macro avg       0.49      0.49      0.49       572\n",
      "weighted avg       0.50      0.51      0.50       572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtener las predicciones con etiquetas reales, para luego hacer an√°lisis m√°s detallado\n",
    "\n",
    "predictions_output_2 = trainer.predict(test_ds)\n",
    "\n",
    "# Obtener predicciones y etiquetas\n",
    "pred_logits = predictions_output_2.predictions  # salida del modelo (logits)\n",
    "pred_labels = np.argmax(pred_logits, axis=1)  # etiquetas predichas (0 o 1)\n",
    "true_labels = predictions_output_2.label_ids    # etiquetas reales\n",
    "\n",
    "# Calcular accuracy (ya lo hace trainer, pero si quieres hacerlo manual)\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# M√°s detalle: reporte clasificaci√≥n\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"Benigno\", \"Maligno\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e3af326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "path_pkl = \"C:/Users/swatc/Desktop/UNI/TFM/TFM/artifacts/models/vit_model_2.pkl\"\n",
    "\n",
    "# with open(path_pkl, \"wb\") as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d654d",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a141ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\swatc/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:01<00:00, 72.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Cargar ResNet50 preentrenada en ImageNet\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Adaptar la primera capa: im√°genes en escala de grises \n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Reemplazar la capa final (FC) por una para clasificaci√≥n binaria\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e02c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "#Data Augmentation\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # si tus im√°genes son RGB pero quieres 1 canal\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # normalizaci√≥n para 1 canal\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c93a13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform y balanceo de clases\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx][\"image\"]\n",
    "        label = int(self.df.iloc[idx][\"label\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # si es PNG en gris, usa .convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cac5ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pesos para clases desbalanceadas\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(df[\"label\"]),\n",
    "    y=df[\"label\"]\n",
    ")\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf45af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train, test, val:\n",
    "\n",
    "# Primero divide en train + temp\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Luego divide df_trainval en train y val\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.2,  # esto ser√° 20% de 80%, es decir, 16% del total\n",
    "    stratify=df_trainval[\"label\"],\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "180b3e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.7287, Train Acc: 0.5131 | Val Loss: 0.7611, Val Acc: 0.5558\n",
      "Epoch 2 | Train Loss: 0.7151, Train Acc: 0.4907 | Val Loss: 0.6962, Val Acc: 0.4639\n",
      "Epoch 3 | Train Loss: 0.7070, Train Acc: 0.5284 | Val Loss: 0.7055, Val Acc: 0.4945\n",
      "Epoch 4 | Train Loss: 0.7061, Train Acc: 0.5038 | Val Loss: 0.7076, Val Acc: 0.4420\n",
      "Epoch 5 | Train Loss: 0.7059, Train Acc: 0.4754 | Val Loss: 0.6937, Val Acc: 0.4748\n",
      "Epoch 6 | Train Loss: 0.7001, Train Acc: 0.5230 | Val Loss: 0.6932, Val Acc: 0.5011\n",
      "Epoch 7 | Train Loss: 0.6950, Train Acc: 0.5334 | Val Loss: 1.7846, Val Acc: 0.4726\n",
      "Epoch 8 | Train Loss: 0.7031, Train Acc: 0.5175 | Val Loss: 0.6965, Val Acc: 0.4945\n",
      "Epoch 9 | Train Loss: 0.6991, Train Acc: 0.5060 | Val Loss: 0.6945, Val Acc: 0.4551\n",
      "Epoch 10 | Train Loss: 0.6968, Train Acc: 0.5033 | Val Loss: 0.6943, Val Acc: 0.4573\n"
     ]
    }
   ],
   "source": [
    "#Entrenamiento\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Aseg√∫rate de tener esto ya cargado:\n",
    "# df_train, df_val, df_test\n",
    "\n",
    "# 1. Dataset personalizado\n",
    "train_dataset = CustomImageDataset(df_train, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(df_val, transform=val_transforms)\n",
    "\n",
    "# 2. DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# 3. Modelo y dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# 4. P√©rdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = Adam(resnet.parameters(), lr=1e-4)\n",
    "\n",
    "# 5. Entrenamiento\n",
    "for epoch in range(10):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validaci√≥n\n",
    "    resnet.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = resnet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96661793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.4573\n"
     ]
    }
   ],
   "source": [
    "#Evaluaci√≥n\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "resnet.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = resnet(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da00cc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 0.7342 | Validation Accuracy: 0.5361\n",
      "Epoch 2/15 | Loss: 0.7235 | Validation Accuracy: 0.4858\n",
      "Epoch 3/15 | Loss: 0.7122 | Validation Accuracy: 0.4858\n",
      "Epoch 4/15 | Loss: 0.7088 | Validation Accuracy: 0.5602\n",
      "Epoch 5/15 | Loss: 0.6941 | Validation Accuracy: 0.5361\n",
      "Epoch 6/15 | Loss: 0.6963 | Validation Accuracy: 0.4880\n",
      "Epoch 7/15 | Loss: 0.6865 | Validation Accuracy: 0.5470\n",
      "Epoch 8/15 | Loss: 0.6826 | Validation Accuracy: 0.5252\n",
      "Epoch 9/15 | Loss: 0.6817 | Validation Accuracy: 0.5186\n",
      "Epoch 10/15 | Loss: 0.6850 | Validation Accuracy: 0.5492\n",
      "Epoch 11/15 | Loss: 0.6808 | Validation Accuracy: 0.5011\n",
      "Epoch 12/15 | Loss: 0.6784 | Validation Accuracy: 0.5077\n",
      "Epoch 13/15 | Loss: 0.6839 | Validation Accuracy: 0.4726\n",
      "Epoch 14/15 | Loss: 0.6698 | Validation Accuracy: 0.5164\n",
      "Epoch 15/15 | Loss: 0.6601 | Validation Accuracy: 0.4748\n"
     ]
    }
   ],
   "source": [
    "#resnet 2:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 1. Dividir el dataset\n",
    "df_trainval, df_test = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "df_train, df_val = train_test_split(df_trainval, test_size=0.2, stratify=df_trainval[\"label\"], random_state=42)\n",
    "\n",
    "# 2. Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Para pasar de 1 canal a 3\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalizaci√≥n simple\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# 3. Dataset personalizado\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx][\"image\"]\n",
    "        label = self.df.iloc[idx][\"label\"]\n",
    "        image = Image.open(image_path).convert(\"L\")  # aseg√∫rate de abrir como escala de grises\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 4. Pesos de clase\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(df_train[\"label\"]),\n",
    "    y=df_train[\"label\"]\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# 5. Datasets y loaders\n",
    "train_dataset = CustomImageDataset(df_train, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(df_val, transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# 6. Modelo\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, len(np.unique(df[\"label\"])))\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# 7. Optimizaci√≥n\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n",
    "\n",
    "# 8. Entrenamiento con validaci√≥n\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validaci√≥n\n",
    "    resnet.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = resnet(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Validation Accuracy: {acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2a836ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Test Accuracy: 0.4808\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6000    0.1981    0.2979       318\n",
      "           1     0.4540    0.8346    0.5881       254\n",
      "\n",
      "    accuracy                         0.4808       572\n",
      "   macro avg     0.5270    0.5164    0.4430       572\n",
      "weighted avg     0.5352    0.4808    0.4267       572\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#evaluacion\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Dataset y loader de test\n",
    "test_dataset = CustomImageDataset(df_test, transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 2. Evaluaci√≥n\n",
    "resnet.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = resnet(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 3. M√©tricas\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\n‚úÖ Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 4. Reporte de clasificaci√≥n\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n",
    "\n",
    "# 5. Matriz de confusi√≥n\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(df[\"label\"]), yticklabels=np.unique(df[\"label\"]))\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cd80e",
   "metadata": {},
   "source": [
    "## VGG16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3d9340af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ecad2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1828 im√°genes\n",
      "Val:   457 im√°genes\n",
      "Test:  572 im√°genes\n"
     ]
    }
   ],
   "source": [
    "#Train/test split\n",
    "\n",
    "# 1. Divide en train+val y test (20% test)\n",
    "df_trainval, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Divide trainval en train y val (20% de trainval ‚Üí 16% del total)\n",
    "df_train, df_val = train_test_split(\n",
    "    df_trainval,\n",
    "    test_size=0.2,\n",
    "    stratify=df_trainval[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Resultado:\n",
    "print(f\"Train: {len(df_train)} im√°genes\")\n",
    "print(f\"Val:   {len(df_val)} im√°genes\")\n",
    "print(f\"Test:  {len(df_test)} im√°genes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "430d8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aseg√∫rate de que los labels sean enteros (0 y 1) y categ√≥ricos\n",
    "df_train[\"label\"] = df_train[\"label\"].astype(int)\n",
    "df_val[\"label\"] = df_val[\"label\"].astype(int)\n",
    "df_test[\"label\"] = df_test[\"label\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "382d198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1828 validated image filenames.\n",
      "Found 457 validated image filenames.\n",
      "Found 572 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Aumentaci√≥n de datos solo para train\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Validaci√≥n y test sin aumentos\n",
    "val_test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Generadores\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"label\",            # valores 0 o 1\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"raw\",         # no hace one-hot\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_val,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"raw\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    x_col=\"image\",\n",
    "    y_col=\"label\",\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"raw\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aea860b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajustar etiquetas\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "def try_model_vgg16_binary():\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Congelar todas las capas\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Descongelar el 50% superior\n",
    "    from_index = int(len(base_model.layers) * 0.5)\n",
    "    for layer in base_model.layers[from_index:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Capas superiores\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)  # 1 salida, activaci√≥n sigmoid\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',               # NO categorical\n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff46cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 5s/step - accuracy: 0.5411 - loss: 0.7814 - precision: 0.4797 - recall: 0.3917 - val_accuracy: 0.5558 - val_loss: 0.6871 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 6s/step - accuracy: 0.5302 - loss: 0.6962 - precision: 0.4485 - recall: 0.1860 - val_accuracy: 0.5558 - val_loss: 0.6876 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 6s/step - accuracy: 0.5557 - loss: 0.6894 - precision: 0.4707 - recall: 0.0667 - val_accuracy: 0.5558 - val_loss: 0.6890 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 6s/step - accuracy: 0.5375 - loss: 0.6913 - precision: 0.4957 - recall: 0.0331 - val_accuracy: 0.5558 - val_loss: 0.6919 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 6s/step - accuracy: 0.5674 - loss: 0.6886 - precision: 0.3792 - recall: 0.0055 - val_accuracy: 0.5558 - val_loss: 0.6866 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 6s/step - accuracy: 0.5592 - loss: 0.6873 - precision: 0.2462 - recall: 0.0032 - val_accuracy: 0.5558 - val_loss: 0.6862 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 6s/step - accuracy: 0.5406 - loss: 0.6899 - precision: 0.2377 - recall: 0.0079 - val_accuracy: 0.5558 - val_loss: 0.6870 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 6s/step - accuracy: 0.5591 - loss: 0.6888 - precision: 0.3214 - recall: 0.0030 - val_accuracy: 0.5558 - val_loss: 0.6866 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 6s/step - accuracy: 0.5506 - loss: 0.6890 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.5558 - val_loss: 0.6875 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m58/58\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 6s/step - accuracy: 0.5645 - loss: 0.6878 - precision: 0.4978 - recall: 0.0229 - val_accuracy: 0.5558 - val_loss: 0.6865 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "#ENtrenamiento \n",
    "\n",
    "model = try_model_vgg16_binary()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2d5eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swatc\\anaconda3\\envs\\conda310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.5643 - loss: 0.6848 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Test Accuracy: 0.5559 | Precision: 0.0000 | Recall: 0.0000\n"
     ]
    }
   ],
   "source": [
    "#Evaluaci√≥n\n",
    "results = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {results[1]:.4f} | Precision: {results[2]:.4f} | Recall: {results[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# path_pkl = \"C:/Users/swatc/Desktop/UNI/TFM/TFM/artifacts/models/vit_model_1.pkl\"\n",
    "\n",
    "# with open(path_pkl, \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
